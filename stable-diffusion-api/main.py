"""
Stable Diffusion 3.5 Medium API Server
–õ–æ–∫–∞–ª—å–Ω—ã–π —Å–µ—Ä–≤–µ—Ä –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ Stable Diffusion 3.5 Medium
"""
import asyncio
import base64
import io
import os
from concurrent.futures import ThreadPoolExecutor
from typing import Optional

import torch
from diffusers import StableDiffusionPipeline
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

app = FastAPI(title="Stable Diffusion 3.5 Medium API")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è –ø–∞–π–ø–ª–∞–π–Ω–∞
pipe = None
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"üîß Using device: {device}")

# Thread pool –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –±–ª–æ–∫–∏—Ä—É—é—â–∏—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
executor = ThreadPoolExecutor(max_workers=1)

# –ú–æ–¥–µ–ª—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
# –í–∞—Ä–∏–∞–Ω—Ç—ã:
# - "ByteDance/SDXL-Lightning" - –æ—á–µ–Ω—å –±—ã—Å—Ç—Ä–∞—è (1-4 —à–∞–≥–∞), –∫–æ–º–º–µ—Ä—á–µ—Å–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–æ (CreativeML Open RAIL++-M)
# - "stabilityai/sdxl-turbo" - –±—ã—Å—Ç—Ä–∞—è (1-4 —à–∞–≥–∞), –∫–æ–º–º–µ—Ä—á–µ—Å–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–æ (CreativeML Open RAIL++-M)
# - "runwayml/stable-diffusion-v1-5" - —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è (20-50 —à–∞–≥–æ–≤), –∫–æ–º–º–µ—Ä—á–µ—Å–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–æ (CreativeML Open RAIL-M)
# - "stabilityai/stable-diffusion-3-medium-diffusers" - —Ç—Ä–µ–±—É–µ—Ç HF token, –ù–ï–ö–û–ú–ú–ï–†–ß–ï–°–ö–û–ï –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
MODEL_ID = os.getenv("SD_MODEL_ID", "ByteDance/SDXL-Lightning")  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é: –±—ã—Å—Ç—Ä–∞—è –º–æ–¥–µ–ª—å —Å –∫–æ–º–º–µ—Ä—á–µ—Å–∫–æ–π –ª–∏—Ü–µ–Ω–∑–∏–µ–π
HF_TOKEN = os.getenv("HUGGINGFACE_TOKEN", "")  # –î–ª—è gated –º–æ–¥–µ–ª–µ–π (–Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è Lightning)


class GenerateRequest(BaseModel):
    prompt: str
    reference_image: Optional[str] = None  # Base64 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
    num_inference_steps: int = 28
    guidance_scale: float = 7.0
    width: int = 1024
    height: int = 1024


class GenerateResponse(BaseModel):
    imageUrl: str  # Base64 data URL
    error: Optional[str] = None


def load_model():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å Stable Diffusion 3.5 Medium"""
    global pipe
    if pipe is not None:
        return pipe

    print(f"üì¶ Loading model: {MODEL_ID}")
    print("‚è≥ This may take a few minutes on first run...")

    try:
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –∫–∞–∫–æ–π –ø–∞–π–ø–ª–∞–π–Ω –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –º–æ–¥–µ–ª–∏
        if "sdxl" in MODEL_ID.lower() or "turbo" in MODEL_ID.lower() or "lightning" in MODEL_ID.lower():
            # SDXL –º–æ–¥–µ–ª–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç StableDiffusionXLPipeline
            from diffusers import StableDiffusionXLPipeline
            print("üì¶ Using SDXL pipeline")
            
            # –î–ª—è gated –º–æ–¥–µ–ª–µ–π –Ω—É–∂–µ–Ω —Ç–æ–∫–µ–Ω
            kwargs = {
                "torch_dtype": torch.float16 if device == "cuda" else torch.float32,
            }
            if HF_TOKEN:
                kwargs["token"] = HF_TOKEN
                print("üîë Using Hugging Face token for gated model")
            
            pipe = StableDiffusionXLPipeline.from_pretrained(
                MODEL_ID,
                **kwargs
            )
        elif "stable-diffusion-3" in MODEL_ID.lower():
            # SD 3.5 Medium –∏—Å–ø–æ–ª—å–∑—É–µ—Ç StableDiffusion3Pipeline
            from diffusers import StableDiffusion3Pipeline
            print("üì¶ Using Stable Diffusion 3 pipeline")
            
            kwargs = {
                "torch_dtype": torch.float16 if device == "cuda" else torch.float32,
            }
            if HF_TOKEN:
                kwargs["token"] = HF_TOKEN
                print("üîë Using Hugging Face token for SD 3.5 Medium")
            
            pipe = StableDiffusion3Pipeline.from_pretrained(
                MODEL_ID,
                **kwargs
            )
        else:
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π Stable Diffusion (1.5, 2.1)
            from diffusers import StableDiffusionPipeline
            print("üì¶ Using standard Stable Diffusion pipeline")
            
            pipe = StableDiffusionPipeline.from_pretrained(
                MODEL_ID,
                torch_dtype=torch.float16 if device == "cuda" else torch.float32,
            )
        pipe = pipe.to(device)

        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
        if device == "cuda":
            pipe.enable_attention_slicing()
            pipe.enable_vae_slicing()

        print("‚úÖ Model loaded successfully")
        return pipe
    except Exception as e:
        print(f"‚ùå Error loading model: {e}")
        raise


@app.on_event("startup")
async def startup_event():
    """–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∑–∏—Ç—Å—è –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—Ä–æ—Å–µ (–ª–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞)"""
    print("‚úÖ Server started. Model will be loaded on first request.")


@app.get("/health")
async def health():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–∏—Å–∞"""
    return {
        "status": "ok",
        "model_loaded": pipe is not None,
        "device": device,
    }


@app.post("/generate", response_model=GenerateResponse)
async def generate_image(request: GenerateRequest):
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –ø—Ä–æ–º–ø—Ç—É
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç image-to-image –µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω reference_image
    """
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –µ—â–µ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ (–≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ, —á—Ç–æ–±—ã –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å)
        if pipe is None:
            print("üì¶ Loading model in background thread...")
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º asyncio.to_thread –¥–ª—è –Ω–µ–±–ª–æ–∫–∏—Ä—É—é—â–µ–π –∑–∞–≥—Ä—É–∑–∫–∏
            await asyncio.to_thread(load_model)
            print("‚úÖ Model loaded, proceeding with generation")

        print(f"üé® Generating image with prompt: {request.prompt[:100]}...")
        print(f"üì∑ Has reference image: {request.reference_image is not None}")

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ, —á—Ç–æ–±—ã –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å event loop
        def generate():
            # –û–∫—Ä—É–≥–ª—è–µ–º —Ä–∞–∑–º–µ—Ä—ã –¥–æ –∫—Ä–∞—Ç–Ω—ã—Ö 8 (—Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ Stable Diffusion)
            width = (request.width // 8) * 8
            height = (request.height // 8) * 8
            if width != request.width or height != request.height:
                print(f"‚ö†Ô∏è Adjusted image size from {request.width}x{request.height} to {width}x{height} (must be multiple of 8)")
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            if request.reference_image:
                # Image-to-image —Ä–µ–∂–∏–º
                # –î–µ–∫–æ–¥–∏—Ä—É–µ–º base64 —Ä–µ—Ñ–µ—Ä–µ–Ω—Å
                if request.reference_image.startswith("data:"):
                    # –£–±–∏—Ä–∞–µ–º data URL –ø—Ä–µ—Ñ–∏–∫—Å
                    base64_data = request.reference_image.split(",")[1]
                else:
                    base64_data = request.reference_image

                image_bytes = base64.b64decode(base64_data)
                from PIL import Image
                reference_img = Image.open(io.BytesIO(image_bytes))

                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–æ–º
                print("üì∑ Using reference image (image-to-image mode)")
                return pipe(
                    prompt=request.prompt,
                    num_inference_steps=request.num_inference_steps,
                    guidance_scale=request.guidance_scale,
                    width=width,
                    height=height,
                )
            else:
                # Text-to-image —Ä–µ–∂–∏–º
                print("üìù Text-to-image mode")
                
                # –î–ª—è Turbo/Lightning –º–æ–¥–µ–ª–µ–π –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–µ–Ω—å—à–µ —à–∞–≥–æ–≤ –∏ guidance_scale
                steps = request.num_inference_steps
                guidance = request.guidance_scale
                
                if "turbo" in MODEL_ID.lower():
                    # Turbo —Ä–∞–±–æ—Ç–∞–µ—Ç –ª—É—á—à–µ —Å 1-4 —à–∞–≥–∞–º–∏
                    steps = min(steps, 4)
                    guidance = 0.0  # Turbo –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç guidance
                    print(f"‚ö° Turbo mode: {steps} steps, guidance={guidance}")
                elif "lightning" in MODEL_ID.lower():
                    # Lightning —Ä–∞–±–æ—Ç–∞–µ—Ç –ª—É—á—à–µ —Å 1-4 —à–∞–≥–∞–º–∏
                    steps = min(steps, 4)
                    guidance = 1.0  # Lightning –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–∏–∑–∫–∏–π guidance
                    print(f"‚ö° Lightning mode: {steps} steps, guidance={guidance}")
                
                return pipe(
                    prompt=request.prompt,
                    num_inference_steps=steps,
                    guidance_scale=guidance,
                    width=width,
                    height=height,
                )
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ —á–µ—Ä–µ–∑ asyncio (–Ω–µ –±–ª–æ–∫–∏—Ä—É–µ—Ç event loop)
        result = await asyncio.wait_for(
            asyncio.to_thread(generate),
            timeout=900.0  # 15 –º–∏–Ω—É—Ç —Ç–∞–π–º–∞—É—Ç –Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é (CPU –º–æ–∂–µ—Ç –±—ã—Ç—å –º–µ–¥–ª–µ–Ω–Ω—ã–º)
        )

        # –ü–æ–ª—É—á–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        image = result.images[0]

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ base64
        buffered = io.BytesIO()
        image.save(buffered, format="PNG")
        img_base64 = base64.b64encode(buffered.getvalue()).decode()

        # –§–æ—Ä–º–∏—Ä—É–µ–º data URL
        image_url = f"data:image/png;base64,{img_base64}"

        print("‚úÖ Image generated successfully")
        return GenerateResponse(imageUrl=image_url)

    except Exception as e:
        error_msg = str(e)
        print(f"‚ùå Error generating image: {error_msg}")
        raise HTTPException(status_code=500, detail=error_msg)


if __name__ == "__main__":
    import uvicorn

    port = int(os.getenv("PORT", "7861"))
    uvicorn.run(app, host="0.0.0.0", port=port)

