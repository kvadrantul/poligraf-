"""
Stable Diffusion 3.5 Medium API Server
–õ–æ–∫–∞–ª—å–Ω—ã–π —Å–µ—Ä–≤–µ—Ä –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ Stable Diffusion 3.5 Medium
"""
import asyncio
import base64
import io
import os
from concurrent.futures import ThreadPoolExecutor
from typing import Optional
import sys

import torch
from diffusers import StableDiffusionPipeline
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import multiprocessing
import psutil

app = FastAPI(title="Stable Diffusion 3.5 Medium API")

# ‚ö° –ö–†–ò–¢–ò–ß–ù–û: –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º PyTorch –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –í–°–ï–• —è–¥–µ—Ä CPU
# Mac Mini M4 –∏–º–µ–µ—Ç 10 —è–¥–µ—Ä (4 performance + 6 efficiency)
NUM_CPU_CORES = multiprocessing.cpu_count()
print(f"üîß Detected CPU cores: {NUM_CPU_CORES}")

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è PyTorch (–∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ —è–¥—Ä–∞)
torch.set_num_threads(NUM_CPU_CORES)
torch.set_num_interop_threads(NUM_CPU_CORES)

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è OpenMP/MKL (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã)
os.environ.setdefault("OMP_NUM_THREADS", str(NUM_CPU_CORES))
os.environ.setdefault("MKL_NUM_THREADS", str(NUM_CPU_CORES))
os.environ.setdefault("NUMEXPR_NUM_THREADS", str(NUM_CPU_CORES))

print(f"‚úÖ PyTorch configured to use {NUM_CPU_CORES} threads")
print(f"‚úÖ PyTorch get_num_threads(): {torch.get_num_threads()}")
print(f"‚úÖ PyTorch get_num_interop_threads(): {torch.get_num_interop_threads()}")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è –ø–∞–π–ø–ª–∞–π–Ω–∞
pipe = None
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"üîß Using device: {device}")

# Thread pool –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –±–ª–æ–∫–∏—Ä—É—é—â–∏—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
executor = ThreadPoolExecutor(max_workers=1)

# –ú–æ–¥–µ–ª—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
# –í–∞—Ä–∏–∞–Ω—Ç—ã (–æ—Ç —Å–∞–º–æ–π –ø—Ä–æ—Å—Ç–æ–π –∫ —Å–ª–æ–∂–Ω–æ–π):
# - "CompVis/stable-diffusion-v1-4" - –°–ê–ú–ê–Ø –ü–†–û–°–¢–ê–Ø –±–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å SD 1.4, ~4GB, –∫–æ–º–º–µ—Ä—á–µ—Å–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–æ ‚ö°‚ö°‚ö°‚ö°
# - "SimianLuo/LCM_Dreamshaper_v7" - –û–ß–ï–ù–¨ –ë–´–°–¢–†–ê–Ø SD 1.5 —Å LCM (1-2 —à–∞–≥–∞!), ~4GB, –∫–æ–º–º–µ—Ä—á–µ—Å–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–æ ‚ö°‚ö°‚ö°
# - "runwayml/stable-diffusion-v1-5" - —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è SD 1.5 (20-50 —à–∞–≥–æ–≤), ~4GB, –∫–æ–º–º–µ—Ä—á–µ—Å–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–æ
# - "ByteDance/SDXL-Lightning" - –æ—á–µ–Ω—å –±—ã—Å—Ç—Ä–∞—è SDXL (1-4 —à–∞–≥–∞), ~10GB, –∫–æ–º–º–µ—Ä—á–µ—Å–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–æ
# - "stabilityai/sdxl-turbo" - –±—ã—Å—Ç—Ä–∞—è SDXL (1-4 —à–∞–≥–∞), ~10GB, –∫–æ–º–º–µ—Ä—á–µ—Å–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–æ
# - "stabilityai/stable-diffusion-3-medium-diffusers" - —Ç—Ä–µ–±—É–µ—Ç HF token, –ù–ï–ö–û–ú–ú–ï–†–ß–ï–°–ö–û–ï –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
# –ü—Ä–æ–±—É–µ–º —Å–Ω–∞—á–∞–ª–∞ LCM Dreamshaper, –µ—Å–ª–∏ –Ω–µ –∑–∞–≥—Ä—É–∑–∏—Ç—Å—è - –∏—Å–ø–æ–ª—å–∑—É–µ–º SD 1.4
MODEL_ID = os.getenv("SD_MODEL_ID", "CompVis/stable-diffusion-v1-4")  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é: –°–ê–ú–ê–Ø –ü–†–û–°–¢–ê–Ø –º–æ–¥–µ–ª—å (–≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç)
FALLBACK_MODEL_ID = "CompVis/stable-diffusion-v1-4"  # Fallback –µ—Å–ª–∏ –æ—Å–Ω–æ–≤–Ω–∞—è –Ω–µ –∑–∞–≥—Ä—É–∑–∏—Ç—Å—è
HF_TOKEN = os.getenv("HUGGINGFACE_TOKEN", "")  # –î–ª—è gated –º–æ–¥–µ–ª–µ–π (–Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è Lightning)


class GenerateRequest(BaseModel):
    prompt: str
    reference_image: Optional[str] = None  # Base64 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
    num_inference_steps: int = 28
    guidance_scale: float = 7.0
    width: int = 1024
    height: int = 1024


class GenerateResponse(BaseModel):
    imageUrl: str  # Base64 data URL
    error: Optional[str] = None


def load_model():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å Stable Diffusion 3.5 Medium"""
    global pipe
    if pipe is not None:
        return pipe

    print(f"üì¶ Loading model: {MODEL_ID}")
    print("‚è≥ Checking if model is cached or starts downloading...")
    sys.stdout.flush()

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –º–æ–¥–µ–ª—å –≤ –∫–µ—à–µ
    import os
    cache_path = os.path.expanduser("~/.cache/huggingface/hub")
    model_cache = None
    if "SimianLuo" in MODEL_ID:
        model_cache = os.path.join(cache_path, "models--SimianLuo--LCM_Dreamshaper_v7")
    elif "CompVis" in MODEL_ID:
        model_cache = os.path.join(cache_path, "models--CompVis--stable-diffusion-v1-4")
    
    if model_cache and os.path.exists(model_cache):
        print("‚úÖ Model found in cache, loading from cache...")
        sys.stdout.flush()
    else:
        print("‚ö†Ô∏è Model not in cache, will download from Hugging Face")
        print("‚è±Ô∏è  Monitoring download progress (15 sec timeout if no progress)...")
        sys.stdout.flush()
    
    try:
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –∫–∞–∫–æ–π –ø–∞–π–ø–ª–∞–π–Ω –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –º–æ–¥–µ–ª–∏
        if "sdxl" in MODEL_ID.lower() or "turbo" in MODEL_ID.lower() or "lightning" in MODEL_ID.lower():
            # SDXL –º–æ–¥–µ–ª–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç StableDiffusionXLPipeline
            from diffusers import StableDiffusionXLPipeline
            print("üì¶ Using SDXL pipeline")
            
            # –î–ª—è gated –º–æ–¥–µ–ª–µ–π –Ω—É–∂–µ–Ω —Ç–æ–∫–µ–Ω
            kwargs = {
                "torch_dtype": torch.float16 if device == "cuda" else torch.float32,
            }
            if HF_TOKEN:
                kwargs["token"] = HF_TOKEN
                print("üîë Using Hugging Face token for gated model")
            
            pipe = StableDiffusionXLPipeline.from_pretrained(
                MODEL_ID,
                **kwargs
            )
        elif "stable-diffusion-3" in MODEL_ID.lower():
            # SD 3.5 Medium –∏—Å–ø–æ–ª—å–∑—É–µ—Ç StableDiffusion3Pipeline
            from diffusers import StableDiffusion3Pipeline
            print("üì¶ Using Stable Diffusion 3 pipeline")
            
            kwargs = {
                "torch_dtype": torch.float16 if device == "cuda" else torch.float32,
            }
            if HF_TOKEN:
                kwargs["token"] = HF_TOKEN
                print("üîë Using Hugging Face token for SD 3.5 Medium")
            
            pipe = StableDiffusion3Pipeline.from_pretrained(
                MODEL_ID,
                **kwargs
            )
        else:
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π Stable Diffusion (1.5, 2.1)
            from diffusers import StableDiffusionPipeline
            print("üì¶ Using standard Stable Diffusion pipeline")
            print(f"üì• Loading model: {MODEL_ID}")
            sys.stdout.flush()
            
            # –ü—Ä–æ—Å—Ç–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ - diffusers —Å–∞–º –ø–æ–∫–∞–∂–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å
            # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –≤ –∫–µ—à–µ - –∑–∞–≥—Ä—É–∑–∏—Ç—Å—è –±—ã—Å—Ç—Ä–æ, –µ—Å–ª–∏ –Ω–µ—Ç - –Ω–∞—á–Ω–µ—Ç —Å–∫–∞—á–∏–≤–∞—Ç—å
            print("‚è≥ Loading model (from cache or downloading)...")
            sys.stdout.flush()
            
            try:
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å (diffusers –ø–æ–∫–∞–∂–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —á–µ—Ä–µ–∑ tqdm)
                pipe = StableDiffusionPipeline.from_pretrained(
                    MODEL_ID,
                    torch_dtype=torch.float16 if device == "cuda" else torch.float32,
                )
                print("‚úÖ Model downloaded/loaded successfully")
                sys.stdout.flush()
            except Exception as e:
                error_msg = str(e)
                print(f"‚ùå Error loading model: {error_msg}")
                sys.stdout.flush()
                raise
        pipe = pipe.to(device)

        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è (–¥–ª—è CPU –∏ CUDA)
        # –í–ê–ñ–ù–û: attention_slicing –º–æ–∂–µ—Ç –∑–∞–º–µ–¥–ª—è—Ç—å –Ω–∞ CPU, –ø—Ä–æ–±—É–µ–º –±–µ–∑ –Ω–µ–≥–æ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Å–∫–æ—Ä–æ—Å—Ç–∏
        if device == "cpu":
            # –î–ª—è CPU –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º attention_slicing - –º–æ–∂–µ—Ç –∑–∞–º–µ–¥–ª—è—Ç—å
            # pipe.enable_attention_slicing(1)  # –û—Ç–∫–ª—é—á–µ–Ω–æ –¥–ª—è CPU
            pipe.enable_vae_slicing()  # VAE slicing —ç–∫–æ–Ω–æ–º–∏—Ç –ø–∞–º—è—Ç—å
            print("üîß CPU mode: VAE slicing enabled, attention slicing disabled for speed")
        else:
            # –î–ª—è CUDA –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±–∞
            pipe.enable_attention_slicing(1)
            pipe.enable_vae_slicing()
        
        # –î–ª—è CPU –∏—Å–ø–æ–ª—å–∑—É–µ–º float32 (–Ω–µ float16) - —ç—Ç–æ —É–∂–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –≤—ã—à–µ
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è CPU
        if device == "cpu":
            # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ —è–¥—Ä–∞ (—É–∂–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–æ –≤—ã—à–µ –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ)
            current_threads = torch.get_num_threads()
            print(f"üîß CPU optimizations: attention_slicing, vae_slicing, {current_threads} threads")
            print(f"üîß PyTorch will use {current_threads} CPU cores for inference")

        print("‚úÖ Model loaded successfully")
        return pipe
    except (TimeoutError, Exception) as e:
        error_msg = str(e)
        if "timeout" in error_msg.lower() or "did not start" in error_msg.lower():
            print(f"‚ùå TIMEOUT: Model {MODEL_ID} –Ω–µ –Ω–∞—á–∞–ª–∞ —Å–∫–∞—á–∏–≤–∞—Ç—å—Å—è –∑–∞ 15 —Å–µ–∫—É–Ω–¥")
            print("üîÑ –ü–µ—Ä–µ–∫–ª—é—á–∞—é—Å—å –Ω–∞ –±–æ–ª–µ–µ –ø—Ä–æ—Å—Ç—É—é –º–æ–¥–µ–ª—å: CompVis/stable-diffusion-v1-4")
            sys.stdout.flush()
            
            # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å SD 1.4 - —Å–∞–º–∞—è –ø—Ä–æ—Å—Ç–∞—è –º–æ–¥–µ–ª—å
            try:
                from diffusers import StableDiffusionPipeline
                print("üì¶ Loading fallback model: CompVis/stable-diffusion-v1-4")
                sys.stdout.flush()
                
                pipe = StableDiffusionPipeline.from_pretrained(
                    "CompVis/stable-diffusion-v1-4",
                    torch_dtype=torch.float32,
                )
                pipe = pipe.to(device)
                pipe.enable_attention_slicing(1)
                pipe.enable_vae_slicing()
                
                if device == "cpu":
                    current_threads = torch.get_num_threads()
                    print(f"üîß CPU optimizations: attention_slicing, vae_slicing, {current_threads} threads")
                
                print("‚úÖ Fallback model loaded successfully")
                return pipe
            except Exception as e2:
                print(f"‚ùå Error loading fallback model: {e2}")
                raise Exception(f"Failed to load both {MODEL_ID} and fallback model: {e2}")
        else:
            print(f"‚ùå Error loading model: {e}")
            raise


@app.on_event("startup")
async def startup_event():
    """–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∑–∏—Ç—Å—è –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—Ä–æ—Å–µ (–ª–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞)"""
    print("‚úÖ Server started. Model will be loaded on first request.")


@app.get("/health")
async def health():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–∏—Å–∞"""
    return {
        "status": "ok",
        "model_loaded": pipe is not None,
        "device": device,
    }


@app.post("/generate", response_model=GenerateResponse)
async def generate_image(request: GenerateRequest):
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –ø—Ä–æ–º–ø—Ç—É
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç image-to-image –µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω reference_image
    """
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –µ—â–µ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ (–≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ, —á—Ç–æ–±—ã –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å)
        if pipe is None:
            print("=" * 60)
            print("üì¶ MODEL NOT LOADED - Starting model loading...")
            print("=" * 60)
            process = psutil.Process(os.getpid())
            cpu_before = process.cpu_percent(interval=0.1)
            threads_before = process.num_threads()
            memory_before = process.memory_info().rss / 1024 / 1024
            print(f"üìä BEFORE load_model(): CPU={cpu_before:.1f}%, Threads={threads_before}, Memory={memory_before:.1f}MB")
            sys.stdout.flush()
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º asyncio.to_thread –¥–ª—è –Ω–µ–±–ª–æ–∫–∏—Ä—É—é—â–µ–π –∑–∞–≥—Ä—É–∑–∫–∏
            await asyncio.to_thread(load_model)
            
            cpu_after = process.cpu_percent(interval=0.1)
            threads_after = process.num_threads()
            memory_after = process.memory_info().rss / 1024 / 1024
            print(f"üìä AFTER load_model(): CPU={cpu_after:.1f}%, Threads={threads_after}, Memory={memory_after:.1f}MB")
            print("‚úÖ Model loaded, proceeding with generation")
            print("=" * 60)
            sys.stdout.flush()

        print(f"üé® Generating image with prompt: {request.prompt[:100]}...")
        print(f"üì∑ Has reference image: {request.reference_image is not None}")

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ, —á—Ç–æ–±—ã –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å event loop
        def generate():
            process = psutil.Process(os.getpid())
            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —Å–±—Ä–∞—Å—ã–≤–∞–µ–º –±—É—Ñ–µ—Ä –≤—ã–≤–æ–¥–∞ –¥–ª—è –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –ª–æ–≥–æ–≤
            sys.stdout.flush()
            sys.stderr.flush()
            
            print("=" * 60)
            print("üöÄ GENERATION STARTED")
            print("=" * 60)
            
            # –û–∫—Ä—É–≥–ª—è–µ–º —Ä–∞–∑–º–µ—Ä—ã –¥–æ –∫—Ä–∞—Ç–Ω—ã—Ö 8 (—Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ Stable Diffusion)
            width = ((request.width + 7) // 8) * 8
            height = ((request.height + 7) // 8) * 8
            
            # –î–ª—è SD 1.4/1.5 (–Ω–µ SDXL) –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä 512x512 –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Å–∫–æ—Ä–æ—Å—Ç–∏
            if "sdxl" not in MODEL_ID.lower() and "stable-diffusion-3" not in MODEL_ID.lower():
                width = min(width, 512)  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 512 –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
                height = min(height, 512)
            
            if width != request.width or height != request.height:
                print(f"‚ö†Ô∏è Adjusted image size from {request.width}x{request.height} to {width}x{height} (must be multiple of 8)")
            
            # –õ–æ–≥–∏—Ä—É–µ–º –Ω–∞—á–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
            cpu_percent = process.cpu_percent(interval=0.1)
            num_threads = process.num_threads()
            memory_mb = process.memory_info().rss / 1024 / 1024
            print(f"üìä INITIAL STATE: CPU={cpu_percent:.1f}%, Threads={num_threads}, Memory={memory_mb:.1f}MB")
            print(f"üîß PyTorch threads: {torch.get_num_threads()}")
            print(f"üîß PyTorch interop threads: {torch.get_num_interop_threads()}")
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            if request.reference_image:
                # Image-to-image —Ä–µ–∂–∏–º
                # –î–µ–∫–æ–¥–∏—Ä—É–µ–º base64 —Ä–µ—Ñ–µ—Ä–µ–Ω—Å
                if request.reference_image.startswith("data:"):
                    # –£–±–∏—Ä–∞–µ–º data URL –ø—Ä–µ—Ñ–∏–∫—Å
                    base64_data = request.reference_image.split(",")[1]
                else:
                    base64_data = request.reference_image

                image_bytes = base64.b64decode(base64_data)
                from PIL import Image
                reference_img = Image.open(io.BytesIO(image_bytes))

                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–æ–º
                print("üì∑ Using reference image (image-to-image mode)")
                
                # –î–ª—è —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
                steps = request.num_inference_steps
                guidance = request.guidance_scale
                if "v1-4" in MODEL_ID.lower() or "stable-diffusion-v1-4" in MODEL_ID.lower():
                    steps = min(steps, 10)
                    guidance = 7.5
                    print(f"‚ö°‚ö°‚ö°‚ö° SD 1.4 mode (SIMPLEST!): {steps} steps, guidance={guidance}")
                elif "lcm" in MODEL_ID.lower():
                    steps = min(steps, 2)
                    guidance = 1.0
                    print(f"‚ö°‚ö°‚ö° LCM mode (FASTEST!): {steps} steps, guidance={guidance}")
                
                return pipe(
                    prompt=request.prompt,
                    num_inference_steps=steps,
                    guidance_scale=guidance,
                    width=width,
                    height=height,
                )
            else:
                # Text-to-image —Ä–µ–∂–∏–º
                print("üìù Text-to-image mode")
                
                # –î–ª—è —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
                steps = request.num_inference_steps
                guidance = request.guidance_scale
                
                if "v1-4" in MODEL_ID.lower() or "stable-diffusion-v1-4" in MODEL_ID.lower():
                    # SD 1.4 - —Å–∞–º–∞—è –ø—Ä–æ—Å—Ç–∞—è –º–æ–¥–µ–ª—å, –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ —à–∞–≥–∏ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
                    steps = min(steps, 10)  # –ú–∏–Ω–∏–º—É–º –¥–ª—è –±–∞–∑–æ–≤–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
                    guidance = 7.5  # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π guidance –¥–ª—è SD 1.4
                    print(f"‚ö°‚ö°‚ö°‚ö° SD 1.4 mode (SIMPLEST!): {steps} steps, guidance={guidance}")
                elif "lcm" in MODEL_ID.lower():
                    # LCM –º–æ–¥–µ–ª–∏ —Ä–∞–±–æ—Ç–∞—é—Ç –ª—É—á—à–µ —Å 1-2 —à–∞–≥–∞–º–∏ (—Å–∞–º—ã–µ –±—ã—Å—Ç—Ä—ã–µ!)
                    steps = min(steps, 2)
                    guidance = 1.0  # LCM –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–∏–∑–∫–∏–π guidance
                    print(f"‚ö°‚ö°‚ö° LCM mode (FASTEST!): {steps} steps, guidance={guidance}")
                elif "turbo" in MODEL_ID.lower():
                    # Turbo —Ä–∞–±–æ—Ç–∞–µ—Ç –ª—É—á—à–µ —Å 1-4 —à–∞–≥–∞–º–∏
                    steps = min(steps, 4)
                    guidance = 0.0  # Turbo –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç guidance
                    print(f"‚ö° Turbo mode: {steps} steps, guidance={guidance}")
                elif "lightning" in MODEL_ID.lower():
                    # Lightning —Ä–∞–±–æ—Ç–∞–µ—Ç –ª—É—á—à–µ —Å 1-4 —à–∞–≥–∞–º–∏
                    steps = min(steps, 4)
                    guidance = 1.0  # Lightning –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–∏–∑–∫–∏–π guidance
                    print(f"‚ö° Lightning mode: {steps} steps, guidance={guidance}")
                
                print(f"üìù Calling pipe() with: prompt='{request.prompt[:50]}...', steps={steps}, guidance={guidance}, size={width}x{height}")
                print("‚è≥ Starting inference (this should use CPU cores)...")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º CPU –ø–µ—Ä–µ–¥ –≤—ã–∑–æ–≤–æ–º
                cpu_before = process.cpu_percent(interval=0.1)
                threads_before = process.num_threads()
                print(f"üìä BEFORE pipe(): CPU={cpu_before:.1f}%, Threads={threads_before}")
                
                # –í—ã–∑—ã–≤–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é
                result = pipe(
                    prompt=request.prompt,
                    num_inference_steps=steps,
                    guidance_scale=guidance,
                    width=width,
                    height=height,
                )
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º CPU –ø–æ—Å–ª–µ –≤—ã–∑–æ–≤–∞
                cpu_after = process.cpu_percent(interval=0.1)
                threads_after = process.num_threads()
                print(f"üìä AFTER pipe(): CPU={cpu_after:.1f}%, Threads={threads_after}")
                print("‚úÖ Inference completed")
                
                return result
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ —á–µ—Ä–µ–∑ asyncio (–Ω–µ –±–ª–æ–∫–∏—Ä—É–µ—Ç event loop)
        # –¢–∞–π–º–∞—É—Ç 30 —Å–µ–∫—É–Ω–¥ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
        print("‚è±Ô∏è  Starting generation with 30 second timeout...")
        sys.stdout.flush()
        try:
            result = await asyncio.wait_for(
                asyncio.to_thread(generate),
                timeout=30.0  # 30 —Å–µ–∫—É–Ω–¥ —Ç–∞–π–º–∞—É—Ç –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
            )
            print("‚úÖ Generation completed within timeout")
            sys.stdout.flush()
        except asyncio.TimeoutError:
            print("‚ùå TIMEOUT: Generation exceeded 30 seconds!")
            sys.stdout.flush()
            raise HTTPException(
                status_code=408,
                detail="Image generation timeout (30 seconds). Model may be too slow or not using CPU cores."
            )

        # –ü–æ–ª—É—á–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        print("üì∑ Extracting image from result...")
        image = result.images[0]
        print(f"‚úÖ Image extracted: size={image.size}, mode={image.mode}")

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ base64 —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π —Ä–∞–∑–º–µ—Ä–∞
        print("üîÑ Converting to JPEG and encoding to base64...")
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º JPEG —Å –∫–∞—á–µ—Å—Ç–≤–æ–º 85% –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞ (–≤–º–µ—Å—Ç–æ PNG)
        buffered = io.BytesIO()
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º RGBA –≤ RGB –¥–ª—è JPEG (JPEG –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç—å)
        if image.mode == 'RGBA':
            # –°–æ–∑–¥–∞–µ–º –±–µ–ª—ã–π —Ñ–æ–Ω
            rgb_image = Image.new('RGB', image.size, (255, 255, 255))
            rgb_image.paste(image, mask=image.split()[3])  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–ª—å—Ñ–∞-–∫–∞–Ω–∞–ª –∫–∞–∫ –º–∞—Å–∫—É
            image = rgb_image
        elif image.mode != 'RGB':
            image = image.convert('RGB')
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ JPEG —Å –∫–∞—á–µ—Å—Ç–≤–æ–º 85% –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞
        image.save(buffered, format="JPEG", quality=85, optimize=True)
        img_base64 = base64.b64encode(buffered.getvalue()).decode()
        
        # –õ–æ–≥–∏—Ä—É–µ–º —Ä–∞–∑–º–µ—Ä –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        original_size = len(img_base64)
        print(f"üìè Image size: {original_size} base64 chars ({original_size * 3 // 4} bytes)")

        # –§–æ—Ä–º–∏—Ä—É–µ–º data URL
        image_url = f"data:image/jpeg;base64,{img_base64}"

        print("‚úÖ Image generated successfully")
        return GenerateResponse(imageUrl=image_url)

    except Exception as e:
        error_msg = str(e)
        print(f"‚ùå Error generating image: {error_msg}")
        raise HTTPException(status_code=500, detail=error_msg)


if __name__ == "__main__":
    import uvicorn

    port = int(os.getenv("PORT", "7861"))
    uvicorn.run(app, host="0.0.0.0", port=port)

