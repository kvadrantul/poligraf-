"""
Stable Diffusion 3.5 Medium API Server
–õ–æ–∫–∞–ª—å–Ω—ã–π —Å–µ—Ä–≤–µ—Ä –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ Stable Diffusion 3.5 Medium
"""
import asyncio
import base64
import io
import os
from concurrent.futures import ThreadPoolExecutor
from typing import Optional

import torch
from diffusers import StableDiffusionPipeline
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import multiprocessing

app = FastAPI(title="Stable Diffusion 3.5 Medium API")

# ‚ö° –ö–†–ò–¢–ò–ß–ù–û: –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º PyTorch –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –í–°–ï–• —è–¥–µ—Ä CPU
# Mac Mini M4 –∏–º–µ–µ—Ç 10 —è–¥–µ—Ä (4 performance + 6 efficiency)
NUM_CPU_CORES = multiprocessing.cpu_count()
print(f"üîß Detected CPU cores: {NUM_CPU_CORES}")

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è PyTorch (–∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ —è–¥—Ä–∞)
torch.set_num_threads(NUM_CPU_CORES)
torch.set_num_interop_threads(NUM_CPU_CORES)

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è OpenMP/MKL (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã)
os.environ.setdefault("OMP_NUM_THREADS", str(NUM_CPU_CORES))
os.environ.setdefault("MKL_NUM_THREADS", str(NUM_CPU_CORES))
os.environ.setdefault("NUMEXPR_NUM_THREADS", str(NUM_CPU_CORES))

print(f"‚úÖ PyTorch configured to use {NUM_CPU_CORES} threads")
print(f"‚úÖ PyTorch get_num_threads(): {torch.get_num_threads()}")
print(f"‚úÖ PyTorch get_num_interop_threads(): {torch.get_num_interop_threads()}")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è –ø–∞–π–ø–ª–∞–π–Ω–∞
pipe = None
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"üîß Using device: {device}")

# Thread pool –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –±–ª–æ–∫–∏—Ä—É—é—â–∏—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
executor = ThreadPoolExecutor(max_workers=1)

# –ú–æ–¥–µ–ª—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
# –í–∞—Ä–∏–∞–Ω—Ç—ã (–æ—Ç —Å–∞–º–æ–π –ø—Ä–æ—Å—Ç–æ–π –∫ —Å–ª–æ–∂–Ω–æ–π):
# - "CompVis/stable-diffusion-v1-4" - –°–ê–ú–ê–Ø –ü–†–û–°–¢–ê–Ø –±–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å SD 1.4, ~4GB, –∫–æ–º–º–µ—Ä—á–µ—Å–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–æ ‚ö°‚ö°‚ö°‚ö°
# - "SimianLuo/LCM_Dreamshaper_v7" - –û–ß–ï–ù–¨ –ë–´–°–¢–†–ê–Ø SD 1.5 —Å LCM (1-2 —à–∞–≥–∞!), ~4GB, –∫–æ–º–º–µ—Ä—á–µ—Å–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–æ ‚ö°‚ö°‚ö°
# - "runwayml/stable-diffusion-v1-5" - —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è SD 1.5 (20-50 —à–∞–≥–æ–≤), ~4GB, –∫–æ–º–º–µ—Ä—á–µ—Å–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–æ
# - "ByteDance/SDXL-Lightning" - –æ—á–µ–Ω—å –±—ã—Å—Ç—Ä–∞—è SDXL (1-4 —à–∞–≥–∞), ~10GB, –∫–æ–º–º–µ—Ä—á–µ—Å–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–æ
# - "stabilityai/sdxl-turbo" - –±—ã—Å—Ç—Ä–∞—è SDXL (1-4 —à–∞–≥–∞), ~10GB, –∫–æ–º–º–µ—Ä—á–µ—Å–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–æ
# - "stabilityai/stable-diffusion-3-medium-diffusers" - —Ç—Ä–µ–±—É–µ—Ç HF token, –ù–ï–ö–û–ú–ú–ï–†–ß–ï–°–ö–û–ï –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
MODEL_ID = os.getenv("SD_MODEL_ID", "SimianLuo/LCM_Dreamshaper_v7")  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é: –°–ê–ú–ê–Ø –ë–´–°–¢–†–ê–Ø –º–æ–¥–µ–ª—å (1-2 —à–∞–≥–∞!)
HF_TOKEN = os.getenv("HUGGINGFACE_TOKEN", "")  # –î–ª—è gated –º–æ–¥–µ–ª–µ–π (–Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è Lightning)


class GenerateRequest(BaseModel):
    prompt: str
    reference_image: Optional[str] = None  # Base64 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
    num_inference_steps: int = 28
    guidance_scale: float = 7.0
    width: int = 1024
    height: int = 1024


class GenerateResponse(BaseModel):
    imageUrl: str  # Base64 data URL
    error: Optional[str] = None


def load_model():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å Stable Diffusion 3.5 Medium"""
    global pipe
    if pipe is not None:
        return pipe

    print(f"üì¶ Loading model: {MODEL_ID}")
    print("‚è≥ This may take a few minutes on first run...")

    try:
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –∫–∞–∫–æ–π –ø–∞–π–ø–ª–∞–π–Ω –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –º–æ–¥–µ–ª–∏
        if "sdxl" in MODEL_ID.lower() or "turbo" in MODEL_ID.lower() or "lightning" in MODEL_ID.lower():
            # SDXL –º–æ–¥–µ–ª–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç StableDiffusionXLPipeline
            from diffusers import StableDiffusionXLPipeline
            print("üì¶ Using SDXL pipeline")
            
            # –î–ª—è gated –º–æ–¥–µ–ª–µ–π –Ω—É–∂–µ–Ω —Ç–æ–∫–µ–Ω
            kwargs = {
                "torch_dtype": torch.float16 if device == "cuda" else torch.float32,
            }
            if HF_TOKEN:
                kwargs["token"] = HF_TOKEN
                print("üîë Using Hugging Face token for gated model")
            
            pipe = StableDiffusionXLPipeline.from_pretrained(
                MODEL_ID,
                **kwargs
            )
        elif "stable-diffusion-3" in MODEL_ID.lower():
            # SD 3.5 Medium –∏—Å–ø–æ–ª—å–∑—É–µ—Ç StableDiffusion3Pipeline
            from diffusers import StableDiffusion3Pipeline
            print("üì¶ Using Stable Diffusion 3 pipeline")
            
            kwargs = {
                "torch_dtype": torch.float16 if device == "cuda" else torch.float32,
            }
            if HF_TOKEN:
                kwargs["token"] = HF_TOKEN
                print("üîë Using Hugging Face token for SD 3.5 Medium")
            
            pipe = StableDiffusion3Pipeline.from_pretrained(
                MODEL_ID,
                **kwargs
            )
        else:
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π Stable Diffusion (1.5, 2.1)
            from diffusers import StableDiffusionPipeline
            print("üì¶ Using standard Stable Diffusion pipeline")
            
            pipe = StableDiffusionPipeline.from_pretrained(
                MODEL_ID,
                torch_dtype=torch.float16 if device == "cuda" else torch.float32,
            )
        pipe = pipe.to(device)

        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è (–¥–ª—è CPU –∏ CUDA)
        pipe.enable_attention_slicing(1)  # –í–∫–ª—é—á–∞–µ–º –¥–ª—è CPU —Ç–æ–∂–µ - —ç–∫–æ–Ω–æ–º–∏—Ç –ø–∞–º—è—Ç—å –∏ –º–æ–∂–µ—Ç —É—Å–∫–æ—Ä–∏—Ç—å
        pipe.enable_vae_slicing()  # –í–∫–ª—é—á–∞–µ–º –¥–ª—è CPU - —ç–∫–æ–Ω–æ–º–∏—Ç –ø–∞–º—è—Ç—å
        
        # –î–ª—è CPU –∏—Å–ø–æ–ª—å–∑—É–µ–º float32 (–Ω–µ float16) - —ç—Ç–æ —É–∂–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –≤—ã—à–µ
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è CPU
        if device == "cpu":
            # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ —è–¥—Ä–∞ (—É–∂–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–æ –≤—ã—à–µ –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ)
            current_threads = torch.get_num_threads()
            print(f"üîß CPU optimizations: attention_slicing, vae_slicing, {current_threads} threads")
            print(f"üîß PyTorch will use {current_threads} CPU cores for inference")

        print("‚úÖ Model loaded successfully")
        return pipe
    except Exception as e:
        print(f"‚ùå Error loading model: {e}")
        raise


@app.on_event("startup")
async def startup_event():
    """–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∑–∏—Ç—Å—è –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—Ä–æ—Å–µ (–ª–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞)"""
    print("‚úÖ Server started. Model will be loaded on first request.")


@app.get("/health")
async def health():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–∏—Å–∞"""
    return {
        "status": "ok",
        "model_loaded": pipe is not None,
        "device": device,
    }


@app.post("/generate", response_model=GenerateResponse)
async def generate_image(request: GenerateRequest):
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –ø—Ä–æ–º–ø—Ç—É
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç image-to-image –µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω reference_image
    """
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –µ—â–µ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ (–≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ, —á—Ç–æ–±—ã –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å)
        if pipe is None:
            print("üì¶ Loading model in background thread...")
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º asyncio.to_thread –¥–ª—è –Ω–µ–±–ª–æ–∫–∏—Ä—É—é—â–µ–π –∑–∞–≥—Ä—É–∑–∫–∏
            await asyncio.to_thread(load_model)
            print("‚úÖ Model loaded, proceeding with generation")

        print(f"üé® Generating image with prompt: {request.prompt[:100]}...")
        print(f"üì∑ Has reference image: {request.reference_image is not None}")

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ, —á—Ç–æ–±—ã –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å event loop
        def generate():
            # –û–∫—Ä—É–≥–ª—è–µ–º —Ä–∞–∑–º–µ—Ä—ã –¥–æ –∫—Ä–∞—Ç–Ω—ã—Ö 8 (—Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ Stable Diffusion)
            width = ((request.width + 7) // 8) * 8
            height = ((request.height + 7) // 8) * 8
            
            # –î–ª—è SD 1.4/1.5 (–Ω–µ SDXL) –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä 512x512 –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Å–∫–æ—Ä–æ—Å—Ç–∏
            if "sdxl" not in MODEL_ID.lower() and "stable-diffusion-3" not in MODEL_ID.lower():
                width = min(width, 512)  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 512 –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
                height = min(height, 512)
            
            if width != request.width or height != request.height:
                print(f"‚ö†Ô∏è Adjusted image size from {request.width}x{request.height} to {width}x{height} (must be multiple of 8)")
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            if request.reference_image:
                # Image-to-image —Ä–µ–∂–∏–º
                # –î–µ–∫–æ–¥–∏—Ä—É–µ–º base64 —Ä–µ—Ñ–µ—Ä–µ–Ω—Å
                if request.reference_image.startswith("data:"):
                    # –£–±–∏—Ä–∞–µ–º data URL –ø—Ä–µ—Ñ–∏–∫—Å
                    base64_data = request.reference_image.split(",")[1]
                else:
                    base64_data = request.reference_image

                image_bytes = base64.b64decode(base64_data)
                from PIL import Image
                reference_img = Image.open(io.BytesIO(image_bytes))

                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–æ–º
                print("üì∑ Using reference image (image-to-image mode)")
                
                # –î–ª—è —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
                steps = request.num_inference_steps
                guidance = request.guidance_scale
                if "v1-4" in MODEL_ID.lower() or "stable-diffusion-v1-4" in MODEL_ID.lower():
                    steps = min(steps, 10)
                    guidance = 7.5
                    print(f"‚ö°‚ö°‚ö°‚ö° SD 1.4 mode (SIMPLEST!): {steps} steps, guidance={guidance}")
                elif "lcm" in MODEL_ID.lower():
                    steps = min(steps, 2)
                    guidance = 1.0
                    print(f"‚ö°‚ö°‚ö° LCM mode (FASTEST!): {steps} steps, guidance={guidance}")
                
                return pipe(
                    prompt=request.prompt,
                    num_inference_steps=steps,
                    guidance_scale=guidance,
                    width=width,
                    height=height,
                )
            else:
                # Text-to-image —Ä–µ–∂–∏–º
                print("üìù Text-to-image mode")
                
                # –î–ª—è —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
                steps = request.num_inference_steps
                guidance = request.guidance_scale
                
                if "v1-4" in MODEL_ID.lower() or "stable-diffusion-v1-4" in MODEL_ID.lower():
                    # SD 1.4 - —Å–∞–º–∞—è –ø—Ä–æ—Å—Ç–∞—è –º–æ–¥–µ–ª—å, –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ —à–∞–≥–∏ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
                    steps = min(steps, 10)  # –ú–∏–Ω–∏–º—É–º –¥–ª—è –±–∞–∑–æ–≤–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
                    guidance = 7.5  # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π guidance –¥–ª—è SD 1.4
                    print(f"‚ö°‚ö°‚ö°‚ö° SD 1.4 mode (SIMPLEST!): {steps} steps, guidance={guidance}")
                elif "lcm" in MODEL_ID.lower():
                    # LCM –º–æ–¥–µ–ª–∏ —Ä–∞–±–æ—Ç–∞—é—Ç –ª—É—á—à–µ —Å 1-2 —à–∞–≥–∞–º–∏ (—Å–∞–º—ã–µ –±—ã—Å—Ç—Ä—ã–µ!)
                    steps = min(steps, 2)
                    guidance = 1.0  # LCM –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–∏–∑–∫–∏–π guidance
                    print(f"‚ö°‚ö°‚ö° LCM mode (FASTEST!): {steps} steps, guidance={guidance}")
                elif "turbo" in MODEL_ID.lower():
                    # Turbo —Ä–∞–±–æ—Ç–∞–µ—Ç –ª—É—á—à–µ —Å 1-4 —à–∞–≥–∞–º–∏
                    steps = min(steps, 4)
                    guidance = 0.0  # Turbo –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç guidance
                    print(f"‚ö° Turbo mode: {steps} steps, guidance={guidance}")
                elif "lightning" in MODEL_ID.lower():
                    # Lightning —Ä–∞–±–æ—Ç–∞–µ—Ç –ª—É—á—à–µ —Å 1-4 —à–∞–≥–∞–º–∏
                    steps = min(steps, 4)
                    guidance = 1.0  # Lightning –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–∏–∑–∫–∏–π guidance
                    print(f"‚ö° Lightning mode: {steps} steps, guidance={guidance}")
                
                return pipe(
                    prompt=request.prompt,
                    num_inference_steps=steps,
                    guidance_scale=guidance,
                    width=width,
                    height=height,
                )
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ —á–µ—Ä–µ–∑ asyncio (–Ω–µ –±–ª–æ–∫–∏—Ä—É–µ—Ç event loop)
        result = await asyncio.wait_for(
            asyncio.to_thread(generate),
            timeout=900.0  # 15 –º–∏–Ω—É—Ç —Ç–∞–π–º–∞—É—Ç –Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é (CPU –º–æ–∂–µ—Ç –±—ã—Ç—å –º–µ–¥–ª–µ–Ω–Ω—ã–º)
        )

        # –ü–æ–ª—É—á–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        image = result.images[0]

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ base64 —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π —Ä–∞–∑–º–µ—Ä–∞
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º JPEG —Å –∫–∞—á–µ—Å—Ç–≤–æ–º 85% –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞ (–≤–º–µ—Å—Ç–æ PNG)
        buffered = io.BytesIO()
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º RGBA –≤ RGB –¥–ª—è JPEG (JPEG –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç—å)
        if image.mode == 'RGBA':
            # –°–æ–∑–¥–∞–µ–º –±–µ–ª—ã–π —Ñ–æ–Ω
            rgb_image = Image.new('RGB', image.size, (255, 255, 255))
            rgb_image.paste(image, mask=image.split()[3])  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–ª—å—Ñ–∞-–∫–∞–Ω–∞–ª –∫–∞–∫ –º–∞—Å–∫—É
            image = rgb_image
        elif image.mode != 'RGB':
            image = image.convert('RGB')
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ JPEG —Å –∫–∞—á–µ—Å—Ç–≤–æ–º 85% –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞
        image.save(buffered, format="JPEG", quality=85, optimize=True)
        img_base64 = base64.b64encode(buffered.getvalue()).decode()
        
        # –õ–æ–≥–∏—Ä—É–µ–º —Ä–∞–∑–º–µ—Ä –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        original_size = len(img_base64)
        print(f"üìè Image size: {original_size} base64 chars ({original_size * 3 // 4} bytes)")

        # –§–æ—Ä–º–∏—Ä—É–µ–º data URL
        image_url = f"data:image/jpeg;base64,{img_base64}"

        print("‚úÖ Image generated successfully")
        return GenerateResponse(imageUrl=image_url)

    except Exception as e:
        error_msg = str(e)
        print(f"‚ùå Error generating image: {error_msg}")
        raise HTTPException(status_code=500, detail=error_msg)


if __name__ == "__main__":
    import uvicorn

    port = int(os.getenv("PORT", "7861"))
    uvicorn.run(app, host="0.0.0.0", port=port)

